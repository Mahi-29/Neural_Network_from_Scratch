# Neural Networks from Scratch

This repository contains my implementation of neural networks from scratch. The goal of this project is to deepen my understanding of the fundamental concepts behind neural networks by building them without relying on high-level machine learning libraries.

## Features
- Implementation of basic neural network components:
    - Forward propagation
    - Backpropagation
    - Gradient descent
- Support for various activation functions (e.g., ReLU, Sigmoid, Tanh).
- Training and evaluation on simple datasets.

## Objectives
- Gain a deeper understanding of how neural networks work under the hood.
- Learn the mathematical foundations of neural networks.
- Build a foundation for implementing more advanced machine learning models.

## Requirements
- Python 3.7 or higher
- Basic knowledge of linear algebra and calculus
- Familiarity with Python programming
- Libraries:
    - Matplotlib
    - Pandas
    - Scikit-learn
- NumPy

## Usage
1. Clone the repository:
     ```bash
     git clone https://github.com/your-username/nn_from_scratch.git
     cd nn_from_scratch
     ```
2. Install dependencies:
     ```bash
     pip install -r requirements.txt
     ```
3. Run the examples:
     ```bash
     python examples/example1.py
     ```

## Roadmap
- [x] Implement basic feedforward neural networks.
- [ ] Add support for different optimization algorithms (e.g., Adam, RMSProp).
- [ ] Experiment with more complex datasets.
- [ ] Add visualization tools for training progress.

## Contributing
Feel free to fork this repository and submit pull requests. Suggestions and improvements are welcome!

